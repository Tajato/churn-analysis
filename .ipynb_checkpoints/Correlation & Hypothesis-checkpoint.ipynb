{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bae2613-be14-4c38-814d-de0cc2541745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libaries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import confusion_matrix, classification_report \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "#Load dataset\n",
    "customers = pd.read_csv('customers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cec6ce-1f5e-4c5d-ae69-60acd4181305",
   "metadata": {},
   "source": [
    "### What features are most likely associated with churn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a074b593-0670-45d3-b927-cc5e63a806f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "churn               1.000000\n",
      "internetService     0.316350\n",
      "monthlycharges      0.192858\n",
      "paperlessBilling    0.191454\n",
      "streamingTV         0.164509\n",
      "streamingMovies     0.162672\n",
      "seniorcitizen       0.150541\n",
      "deviceProtection    0.084402\n",
      "onlineBackup        0.073934\n",
      "multipleLines       0.036148\n",
      "techSupport         0.026744\n",
      "onlineSecurity      0.023014\n",
      "phoneService        0.011691\n",
      "totalcharges       -0.199484\n",
      "paymentMethod      -0.262918\n",
      "tenure             -0.354049\n",
      "Name: churn, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#We will answer this by finding the correlation coefficients in regards to churn. But first.\n",
    "\n",
    "# Convert categorical variables to numerical variables\n",
    "customers['churn'] = customers['churn'].map({'No': 0, 'Yes': 1})\n",
    "customers['phoneService'] = customers['phoneservice'].map({'No': 0, 'Yes': 1})\n",
    "customers['multipleLines'] = customers['multiplelines'].map({'No phone service': 0, 'No': 1, 'Yes': 2})\n",
    "customers['internetService'] = customers['internetservice'].map({'No': 0, 'DSL': 1, 'Fiber optic': 2})\n",
    "customers['onlineSecurity'] = customers['onlinesecurity'].map({'No internet service': 0, 'No': 1, 'Yes': 2})\n",
    "customers['onlineBackup'] = customers['onlinebackup'].map({'No internet service': 0, 'No': 1, 'Yes': 2})\n",
    "customers['deviceProtection'] = customers['deviceprotection'].map({'No internet service': 0, 'No': 1, 'Yes': 2})\n",
    "customers['techSupport'] = customers['techsupport'].map({'No internet service': 0, 'No': 1, 'Yes': 2})\n",
    "customers['streamingTV'] = customers['streamingtv'].map({'No internet service': 0, 'No': 1, 'Yes': 2})\n",
    "customers['streamingMovies'] = customers['streamingmovies'].map({'No internet service': 0, 'No': 1, 'Yes': 2})\n",
    "customers['paperlessBilling'] = customers['paperlessbilling'].map({'No': 0, 'Yes': 1})\n",
    "customers['paymentMethod'] = customers['paymentmethod'].map({'Electronic check': 0, 'Mailed check': 1, 'Bank transfer (automatic)': 2, 'Credit card (automatic)': 3})\n",
    "\n",
    "# Calculate the correlation coefficients\n",
    "#This is sorted so the highest correlation coeff are on top\n",
    "correlations = customers.corr()['churn'].sort_values(ascending=False)\n",
    "\n",
    "# Print the results\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c1a9ab-a7b7-46d3-8b94-52e707389a02",
   "metadata": {},
   "source": [
    "So, based on this, internetservice, monthlycharges and paperless billing are mostly associated with churn for positive values.\n",
    "However, 0.3 is moderate and not very strong, therefore, there is absolutely other factors affecting churn which we will explore later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed22d640-947c-44f1-ad9c-727c147194ca",
   "metadata": {},
   "source": [
    "In our SQL script, we found that customers with a higher monthly charge, they churned more. So, to solidify this association,\n",
    "we will perform hypothesis testing to determine if the correlation between monthly charges and churn is due to chance or if there is a statistical significance between the two. We will use Pearson's correlation test to determine this. This test will tell us if there is a linear relationship between the two and if this relationship is indeed significant. The result will be the coefficient and the p-value. The p-value will tell us everything we need to know about our test.\n",
    "Null Hypothesis: Our null hypothesis will be a statement that states that there is no significance between the two variables or simply that the relationship is merely due to chance. Therefore, our null hypothesis is: There is no significant correlation between the two variables(monthly charges and churn).\n",
    "\n",
    "Alternative Hypothesis: The alternative hypothesis basically does the opposite and states that there is a significance between the two variables and their relationship is not due to chance. Therefore, our alternative hypothesis is: There is a significant correlation between the two variables.\n",
    "\n",
    "Now as I said earlier, the p-value tells us everything we need to know. Therefore, if the p-value is less than the significance level(0.05), we can reject the null hypothesis. If the p-value is greater than the significance level, we failed to reject the null hypothesis and we would have to accept it.\n",
    "\n",
    "Let us see what our p value is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9bb9e32-cf7b-42f7-a327-3d274f6c3bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient: 0.19285821847007886\n",
      "P-value: 6.760843118056653e-60\n"
     ]
    }
   ],
   "source": [
    "# Load monthly charges and churn into columns\n",
    "mont_charges = customers['monthlycharges']\n",
    "churn = customers['churn']\n",
    "\n",
    "# Perform Pearson correlation test\n",
    "corr, p_value = pearsonr(mont_charges, churn)\n",
    "\n",
    "# Print results\n",
    "print(\"Correlation coefficient:\", corr)\n",
    "print(\"P-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e747a5f-5210-4271-a787-d876228256be",
   "metadata": {},
   "source": [
    "This is a very small p-value and therefore, once the p value is less than the significance level of 0.05, then we can reject the null hypothesis and this means that there is a significant relationship between monthlycharges and churn. It is not due to chance.\n",
    "\n",
    "Therefore, customers with higher monthly charges, are at-risk of churning. I would recommend revising pricing of services to ensure it is reasonable for all customers because customers may be leaving because of high monthly charges. So, lowering prices may retain some customers and reduce the churn rate. \n",
    "Also, conduct a customer survey to find out the specifics to see if customers are churning due to high monthly charges because of the price not matching up to the services being offered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e00f414-ce9c-4ee9-8e80-bfb311e35444",
   "metadata": {},
   "source": [
    "### Let's now predict customer churn using Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "669c0249-466e-4d2e-a128-b11198fc3451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are dropping customerID and gender because they're not relevant to our model. And we're dropping totalcharges because\n",
    "#it is highly correlated to tenure, so keeping it would be redundant and cause multilinearity issues.\n",
    "to_drop = ['customerID','gender','totalcharges']\n",
    "customers = customers.drop(to_drop,axis=1)\n",
    "categorical_cols = ['deviceprotection','onlinebackup','multiplelines','phoneservice','onlinesecurity','techsupport','internetservice', 'contract','tenure','paymentmethod','paperlessbilling','streamingtv','streamingmovies','partner','dependents']\n",
    "encoded_features = pd.get_dummies(customers, columns=categorical_cols)\n",
    "features = encoded_features.drop('churn', axis=1)\n",
    "target = encoded_features['churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "369fba2e-7796-4f1a-a191-34ae8f80fc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.84      0.89      0.86      1549\n",
      "         Yes       0.63      0.52      0.57       561\n",
      "\n",
      "    accuracy                           0.79      2110\n",
      "   macro avg       0.73      0.71      0.72      2110\n",
      "weighted avg       0.78      0.79      0.78      2110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "\n",
    "#Increase the number of iterations\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fit the model to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# I will now evaluate the model \n",
    "y_pred = logreg.predict(X_test) \n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d9d3f-a432-4953-a488-981fbdcf2cd3",
   "metadata": {},
   "source": [
    "Nice! So after printing out the classification report, we are interested in the precision, accuracy, f1-score and recall.\n",
    "\n",
    "The accuracy for our model is 0.79. This means that our model correctly predicts whether a customer will churn or not and is correct about 79% of the time.\n",
    "\n",
    "The precision for yes is 0.63. This means that when the model predicts if customers will churn, it is correct about it 63% of the time.\n",
    "The precision for no is 0.84. This means that when the model predicts if a customer won't churn, it is correct about it 84% of the time.\n",
    "\n",
    "The recall is for YES is 0.52. This means that the model correctly identifies 52% of the customers who actually churned.\n",
    "The recall for NO is 0.89. This means that the model correctly identifies 89% of the customers who did not churned.\n",
    "\n",
    "The F-1 score for NO is 0.87 and the for YES, it is 0.57. The F-1 score takes into account precision and recall. It is the harmonic mean of precision and recall. A f-1 score of 0.87 means the model is doing pretty well at predicting who will not churn. However, the f-1 score for predicting if they will churn is 0.57, this is low compared to the 0.87. So, this version of the model does better at predicting who will not churn versus who will.\n",
    "The weight avg is average of precision, recall and f-1 score across both categories. The model's weight avg is 0.78. which means the model is doing pretty well, but it still could be improved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cdf19207-e420-4875-af90-445095c23d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['customerID','gender','totalcharges']\n",
    "customers = customers.drop(to_drop,axis=1)\n",
    "categorical_cols = ['deviceprotection','onlinebackup','multiplelines','phoneservice','onlinesecurity','techsupport','internetservice', 'contract','tenure','paymentmethod','paperlessbilling','streamingtv','streamingmovies','partner','dependents']\n",
    "encoded_features = pd.get_dummies(customers, columns=categorical_cols)\n",
    "features = encoded_features.drop('churn', axis=1)\n",
    "target = encoded_features['churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "732f4598-d188-44aa-82db-cca0096b66e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.87      0.78      0.82      1549\n",
      "         Yes       0.53      0.67      0.59       561\n",
      "\n",
      "    accuracy                           0.75      2110\n",
      "   macro avg       0.70      0.73      0.71      2110\n",
      "weighted avg       0.78      0.75      0.76      2110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "\n",
    "#Increase the number of iterations\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "# Apply recursive feature elimination to select the most important features, so we can improve the model's performance\n",
    "rfe = RFE(logreg, num_features=10)\n",
    "rfe.fit(X_train, y_train)\n",
    "rfe_features = X_train.columns[rfe.support_]\n",
    "\n",
    "# Train the logistic regression model on the new features \n",
    "logreg.fit(X_train[rfe_features], y_train)\n",
    "\n",
    "# Make predictions on the testing data and evaluate the model\n",
    "y_pred = logreg.predict(X_test[rfe_features])\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5937b74c-8283-467e-be5f-cd62ea13790b",
   "metadata": {},
   "source": [
    "Based on the new classification report, the model did improve based on what I wanted. My goal was to create a model that would do a better job at predicting those who will churn. The recall and f1-score improved. However, there is still some downsides. The precision decreased from 63% to 53%. The model now correctly identifies 67% of customers who actually churned. This is higher than the 52% in the previous model. This means that this model is better at identifying potential churners, which is what I wanted. However, this model is doing a worse job at being correct about who churned. It is now correct 53%  of the time when predicting customers who churn. Before, it was correct 63% of the time. It does do a better job at identifying potential churners though and that is the goal.   \n",
    "Now, because we used the RFE for feature selection, the accuracy decreased. RFE selects the most important features but it does not always select the most optimal features.The model was trained on a smaller set of features. The accuracy decreased, it is less than the accuracy score we saw before, but it is still decent.\n",
    "\n",
    "While the second model with RFE does better at identifying customers who actually churned, it has a lower precision. Therefore, the second model would end up predicting customers to churn who don't actually end up churning. Therefore, if a company was to use this model, it would end up spending to retain customers who end up not churning.\n",
    "\n",
    "On the other hand, the first model has a higher precision. Therefore, it would have as few false positives(customers who are predicted to churn but don't) as possible, but it would end up missing out on some true positives(actual churners). \n",
    "\n",
    "So, if a company uses the first model, you wouldn't overspend by trying to retain customers who won't actually churn but you would still end up losing money because there will be some customers that are going to churn that the model did not predict.\n",
    "\n",
    "While, in the second model, you would overspend by trying to retain customers who won't actually churn, but you would still be identifying actual churners. Because this model(second model) identifies 67% of customers who actually churned versus the first model, which identifies 52% of customers who actually churned. So both models have their pros and cons. \n",
    "\n",
    "To conclude, it boils down to the specific goals of the company. In this project, the goal of the company is the minimize the customers who churn even if it means that some customers who will not actually churn would be included. Therefore, the second model would work perfectly for our needs.\n",
    "\n",
    "On the other hand, there may be another company that would like to minimize the number of erroneous churn predictions so they don't have to utilize unnecessary resources. In this case, the first model would be better because the precision is higher so it will more accurately predict customers who will actually churn.\n",
    "The optimal situation would be do have a balance between precision and recall. To have a model that accurately predicts customers who will churn and could identify a decent percentage of customers who actually churned.\n",
    "We could still tune the first model by choosing another classification model other than Logistic Regression. We could apply undersampling or oversampling. One method to tune the model was to use a feature selection algorithm, so I used Recursive Feature Elimination to choose the best features to train for the model in hopes it would improve the model. This is my first predictive model. I think this is a good start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad94dd83-2a82-4c7b-ae27-55c1905763a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279c935-d41f-49b5-ab7d-3e5bafaaa848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
